{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0cL0d-bWGtN"
      },
      "source": [
        "# Deep Learning\n",
        "# DL02 Tensores PyTorch\n",
        "\n",
        "En este notebook, presentamos  [PyTorch] (http://pytorch.org/), un framework para construir y entrenar redes neuronales. En el calculo de redes neuronales se utilizan frecuentemente matrices y de forma mas general tensores.  PyTorch toma estos tensores y hace que sea simple moverlos a GPU para el procesamiento más rápido necesario al entrenar redes neuronales. También proporciona un módulo que calcula automáticamente los gradientes (¡para propagación hacia atrás!) Y otro módulo específicamente para construir redes neuronales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_yG1qYiN1s-"
      },
      "source": [
        "## <font color='blue'>**Cosas interesantes que han logrado las redes neuronales**</font>\n",
        "<p style='text-align: justify;'>\n",
        "\n",
        "\n",
        "1.  Las redes han superados a los mejores jugadores de Go y Starcraft.\n",
        "\n",
        "Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484-489.\n",
        "\n",
        "2. El problema es predecir la siguiente palabra dadas las palabras anteriores. La tarea es fundamental para el reconocimiento óptico de caracteres o el habla, y también se utiliza para la corrección ortográfica, el reconocimiento de escritura a mano y la traducción automática estadística.\n",
        "\n",
        "https://github.com/oxford-cs-deepnlp-2017/lectures/blob/master/Lecture%2010%20-%20Text%20to%20Speech.pdf\n",
        "\n",
        "3. Sistemas de respuesta a preguntas que intentan responder la consulta de un usuario que se formula en forma de pregunta devolviendo la frase none adecuada, como una ubicación, una persona o una fecha.\n",
        "\n",
        "https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b\n",
        "\n",
        "4. La detección de objetos es la tarea de la clasificación de imágenes con localización, aunque una imagen puede contener múltiples objetos que requieren localización y clasificación.\n",
        "\n",
        "\n",
        "![Deteccón](https://drive.google.com/uc?export=view&id=12BRJQlZggZiAyHobuN-e-Zv1b1VEAAId)\n",
        "\n",
        "\n",
        "5. La segmentación de objetos, o segmentación semántica, es la tarea de detección de objetos donde se dibuja una línea alrededor de cada objeto detectado en la imagen. La segmentación de imágenes es un problema más general de dividir una imagen en segmentos.\n",
        "\n",
        "![Segmentacion](https://drive.google.com/uc?export=view&id=1Dmy4OOl4MBrO1w4TgTUgP3WxnpLk4i4g)\n",
        "\n",
        "\n",
        "6. La transferencia de estilo o transferencia de estilo neuronal es la tarea de aprender el estilo de una o más imágenes y aplicar ese estilo a una nueva imagen.\n",
        "\n",
        "![Segmentacion](https://drive.google.com/uc?export=view&id=1jT6e5p6vtnBrwTzb63qCsIQuz_lh1IYa)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQWso2ebWGtU"
      },
      "source": [
        "## <font color='blue'>**Redes Neuronales.**</font>\n",
        "\n",
        "\n",
        "Deep Learning se basa en redes neuronales artificiales que han existido de alguna forma desde finales de la década de 1950. Las redes se construyen a partir de partes individuales que se aproximan a las neuronas, típicamente llamadas unidades o simplemente \"neuronas\". Cada unidad tiene un cierto número de entradas ponderadas. Estas entradas ponderadas se suman (una combinación lineal) y luego se pasan a través de una función de activación para obtener la salida de la unidad.\n",
        "\n",
        "![Log](https://drive.google.com/uc?export=view&id=1EBHN-Ho1ZmYoRy1x2ZkER9fLWBSTwvO3)\n",
        "\n",
        "\n",
        "Matematicamente esto se ve de la siguiente forma:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
        "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Si lo expresamos en notacion vectorial esto es básicamente un prodcuto interno entre dos vectores.\n",
        "\n",
        "$$\n",
        "h = \\begin{bmatrix}\n",
        "1, x_1 \\, x_2 \\cdots  x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "           b \\\\\n",
        "           w_1 \\\\\n",
        "           w_2 \\\\\n",
        "           \\vdots \\\\\n",
        "           w_n\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNKIE1wrWGtU"
      },
      "source": [
        "### Tensores la base de los calculos en redes neuronales.\n",
        "\n",
        "Resulta que los cálculos de la red neuronal son solo un montón de operaciones de álgebra lineal de **tensores** (una generalización de las matrices). Un vector es un tensor unidimensional, una matriz es un tensor bidimensional, una matriz con tres índices es un tensor tridimensional (imágenes de color RGB, por ejemplo). La estructura de datos fundamental para las redes neuronales son los tensores y PyTorch (así como casi cualquier otro framework de aprendizaje profundo) se construye alrededor de los tensores.\n",
        "\n",
        "\n",
        "\n",
        "![Tensores](https://drive.google.com/uc?export=view&id=15Fr9h_acKoagEYMgKxFKJ7wJHWrafGlv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSRYaFr2Xbvd"
      },
      "source": [
        "\n",
        "### Exploremos cómo podemos usar PyTorch para construir una red neuronal simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va2B9K3KWGtV"
      },
      "source": [
        "# Primero importemos PyTorch\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TaP8cY8WGtV"
      },
      "source": [
        "# Construyamos\n",
        "def activation(x):\n",
        "    \"\"\" Sigmoid activation function\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x: torch.Tensor\n",
        "    \"\"\"\n",
        "    return 1 / (1 + torch.exp(-x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DECONSTRUCCION\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "activation(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDTcVHAonlRA",
        "outputId": "2515cb03-e2ed-4fb0-e48e-3a9bcfb1988b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7311, 0.8808, 0.9526],\n",
              "        [0.9820, 0.9933, 0.9975]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXL49j2jWGtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d177355f-eff2-4f3f-a3e8-03b1db11863b"
      },
      "source": [
        "# Generemos datos.\n",
        "# Las semillas son importantes para evitar que los experimentos generen datos diferentes\n",
        "torch.manual_seed(7) # Definamos una semilla para poder reproducir el cálculo.\n",
        "\n",
        "# Features\n",
        "# Creamos 5 variables aleatoriamente, distribuidas de forma normal\n",
        "# Acá vemos una estratégia de normalidad para seleccionar los pesos iniciales\n",
        "# La inicialización puede hacer que el modelo converja a valores distintos\n",
        "features = torch.randn((1, 5))\n",
        "print(features, '\\n')\n",
        "# Retorna pesos utilizando una distribución normal con media 0 y variana 1.\n",
        "weights = torch.randn_like(features)\n",
        "print(weights, '\\n')\n",
        "#  Adicionalmente definimos un bias (b)\n",
        "bias = torch.randn((1, 1))\n",
        "print(bias)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]]) \n",
            "\n",
            "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]]) \n",
            "\n",
            "tensor([[0.3177]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh5yLBbJWGtW"
      },
      "source": [
        "Arriba se generaron  datos que podemos usar para obtener la salida de nuestra red simple. Todo esto es aleatorio por ahora, en adelante comenzaremos a usar datos normales. Analicemos cada línea:\n",
        "\n",
        "`features = torch.randn((1, 5))` crea un tensor de forma `(1, 5)`, 1 fila y 5 columnas, este vector contiene valores aleatoriamente distribuidos de acuerdo a una distribución normal con media 0 y desviación estandar 1.\n",
        "\n",
        "`weights = torch.randn_like(features)` crea otro tensor con la misma forma que `features`, y nuevamente conteniendo valores de una distribución normal.\n",
        "\n",
        "Finalmente , `bias = torch.randn((1, 1))` crea un unico valor obtenido de una distribución normal.\n",
        "\n",
        "Los tensores PyTorch se pueden agregar, multiplicar, restar, etc., al igual que las matrices de Numpy. En general, usará los tensores PyTorch de la misma manera que usaría las matrices Numpy. Sin embargo, vienen con algunos buenos beneficios, como la aceleración de GPU, que veremos más adelante. Por ahora, use los datos generados para calcular la salida de esta red simple de una sola capa.\n",
        "\n",
        "\n",
        "**Ejemplo**: Calcule la salida de la redo considerando como input a `features`, como pesos `weights`, y bias `bias`. PyTorch tiene un método [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum), para calcular sumas. Adicionalmente utilice la función `activation` definida  anteriormente como función de activación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9qSiEOwWGtW",
        "outputId": "5007bd36-e14e-47a7-ab17-d8535e4cf9c1"
      },
      "source": [
        "### Solucion\n",
        "# Sol 1. Multiplicamos los vectores y sumamos el bias\n",
        "# POdemos hacerlo de dos formas; una más optimizada que la otra\n",
        "%timeit y = activation(torch.sum(features * weights) + bias)\n",
        "#print(y)\n",
        "# Sol 2\n",
        "%timeit y = activation((features * weights).sum() + bias)\n",
        "#print(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53.7 µs ± 5.89 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "49.6 µs ± 9.12 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MpegDAWGtX"
      },
      "source": [
        "Sin embargo, puedes hacer la multiplicación y la suma en la misma operación usando una multiplicación matricial. En general, querrás usar multiplicaciones matriciales ya que son más eficientes y aceleradas usando bibliotecas modernas y computación de alto rendimiento en GPU.\n",
        "\n",
        "Aquí, queremos hacer una multiplicación matricial de las features y los weights. Para esto podemos usar [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul)\n",
        "\n",
        "```python\n",
        ">> torch.mm(features, weights)\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "RuntimeError                              Traceback (most recent call last)\n",
        "<ipython-input-13-15d592eb5279> in <module>()\n",
        "----> 1 torch.mm(features, weights)\n",
        "\n",
        "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
        "```\n",
        "A medida que construye redes neuronales en cualquier framework, este error aparecerá a menudo. Lo que sucede aquí es que nuestros tensores no tienen las formas correctas para realizar una multiplicación matricial. Recuerde que para las multiplicaciones matriciales, El número de columnas en el primer tensor debe ser igual al número de filas en el segundo tensor. Ambas `features` y `weights` tienen la misma forma, `(1, 5)`. Esto significa que necesitamos cambiar la forma de los `weights` para que la multiplicación de la matriz funcione.\n",
        "\n",
        "\n",
        "**Nota:** Para ver la forma de un tensor `tensor`, usamos `tensor.shape`.\n",
        "\n",
        "Existen opciones para cambiar la forma a un tensor: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), y [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
        "\n",
        "* `weights.reshape(a, b)` Retorna un tensor con la misma data que `weights` y con tamaño `(a, b)`.\n",
        "* `weights.resize_(a, b)` Retorna un tensor con distinta forma. Si la nueva forma tiene menos elementos que la original, algunos seran removidos. Si la nueva forma tiene mas elementos que el original, los nuevos elementos serán no inicializados. Leer más [aquí](https://discuss.pytorch.org/t/what-is-in-place-operation/16244).\n",
        "* `weights.view(a, b)` retornará el mismo tensor con la misma data que  `weights` con tamaño `(a, b)`.\n",
        "\n",
        "> **Ejemplo**: Calcule la salida de nuestra pequeña red usando la multiplicación de matrices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DECONSTRUCCION\n",
        "w = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(w)\n",
        "print(w.shape)\n",
        "w_rsh = w.reshape(3, 2)\n",
        "print(w_rsh)\n",
        "w_res = w.resize_(3, 3)\n",
        "print(w_res)\n",
        "print(w) # Las modificaciones afectan al objeto original 'w'\n",
        "print(w.view(1, 9))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THzhRkkTtYyh",
        "outputId": "f735b994-0a85-4a49-b26c-b25c2b026a56"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "tensor([[              1,               2,               3],\n",
            "        [              4,               5,               6],\n",
            "        [      154509376, 134873430982720,       154509376]])\n",
            "tensor([[              1,               2,               3],\n",
            "        [              4,               5,               6],\n",
            "        [      154509376, 134873430982720,       154509376]])\n",
            "tensor([[              1,               2,               3,               4,\n",
            "                       5,               6,       154509376, 134873430982720,\n",
            "               154509376]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos el error\n",
        "y = activation(torch.mm(features, weights) + bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "rGL_k3YJ63dm",
        "outputId": "b3a90808-508a-45b6-f905-47249aa200a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6eedf49a809d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Veamos el error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(features.shape)\n",
        "print(weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNiMoMo1vygw",
        "outputId": "2f682be1-1121-44bb-dd2f-e37d52ceb69a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foxGeXdDWGtY"
      },
      "source": [
        "## Solucion\n",
        "# mm es multiplicación de matrices\n",
        "y = activation(torch.mm(features, weights.view(5,1)) + bias)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H33DCWT4WGtY"
      },
      "source": [
        "### ¡Apilarlos!\n",
        "\n",
        "Así es como puede calcular la salida de una sola neurona. El poder real de este algoritmo ocurre cuando comienzas a apilar estas unidades individuales en capas y pilas de capas, en una red de neuronas. La salida de una capa de neuronas se convierte en la entrada para la siguiente capa. Con múltiples unidades de entrada y unidades de salida, ahora necesitamos expresar los pesos como una matriz.\n",
        "\n",
        "![Log](https://drive.google.com/uc?export=view&id=1baAB8q9xxML3osQFAHfWdQQPzpMTEf-G)\n",
        "\n",
        "\n",
        "La primera capa que se muestra en la parte inferior aquí son las entradas, llamadas **capa de entrada**. La capa intermedia se llama __capa oculta__, y la capa final (a la derecha) es la **capa de salida**. Podemos expresar esta red matemáticamente con matrices nuevamente y usar la multiplicación de matrices para obtener combinaciones lineales para cada unidad en una operación. Por ejemplo, la capa oculta ($ h_1 $ y $ h_2 $ aquí) se puede calcular\n",
        "\n",
        "$$\n",
        "\\vec{h} = [h_1 \\, h_2] =\n",
        "\\begin{bmatrix}\n",
        "x_1 \\, x_2 \\cdots \\, x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "           w_{11} & w_{12} \\\\\n",
        "           w_{21} &w_{22} \\\\\n",
        "           \\vdots &\\vdots \\\\\n",
        "           w_{n1} &w_{n2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "La salida para esta pequeña red se encuentra tratando la capa oculta como entradas para la unidad de salida. La salida de la red se expresa simplemente\n",
        "\n",
        "\n",
        "$$\n",
        "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvle7MhabcmF"
      },
      "source": [
        "### <font color='green'>**Actividad 1**</font>\n",
        "\n",
        "1. Defina una semilla para reproducir el cálculo\n",
        "\n",
        "2. Genere un dataset  con tres variables aleatorias.\n",
        "\n",
        "3. Defina el tensor de pesos aleatorios entre la capa de entrada y la capa hidden.\n",
        "\n",
        "4. Defina un tensor de pesos aleatorios entre la capa hidden y la de salida.\n",
        "\n",
        "5. Defina los tensores bias para la capa hidden y de salida.\n",
        "\n",
        "6. Calcule la salida para esta red multicapa utilizando los pesos `W1` y` W2`, y los sesgos, `B1` y` B2`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definamos una semilla\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# 1. Generamos un dataset con tres variables aleatorias\n",
        "features = torch.randn((1, 3))  # 1 muestra con 3 features\n",
        "print(\"Features:\", features)\n",
        "\n",
        "# Supongamos que la capa oculta tiene 4 neuronas y la salida 1\n",
        "# 3. Pesos entre capa de entrada (3) y capa oculta (4)\n",
        "W1 = torch.randn((3, 4))\n",
        "print(\"W1:\", W1)\n",
        "\n",
        "# 4. Pesos entre capa oculta (4) y capa de salida (1)\n",
        "W2 = torch.randn((4, 1))\n",
        "print(\"W2:\", W2)\n",
        "\n",
        "# 5. Bias de capa oculta (4) y de salida (1)\n",
        "B1 = torch.randn((1, 4))\n",
        "B2 = torch.randn((1, 1))\n",
        "print(\"B1:\", B1)\n",
        "\n",
        "# Forward pass\n",
        "hidden = activation(torch.mm(features, W1) + B1)  # capa oculta\n",
        "output = torch.mm(hidden, W2) + B2               # capa de salida (sin activación final)\n",
        "print(\"Salida de la red:\", output)"
      ],
      "metadata": {
        "id": "17F4QGQAR_Gj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ed3376-a3c3-4eaa-ff5d-27462abe64c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: tensor([[-0.1468,  0.7861,  0.9468]])\n",
            "W1: tensor([[-1.1143,  1.6908, -0.8948, -0.3556],\n",
            "        [ 1.2324,  0.1382, -1.6822,  0.3177],\n",
            "        [ 0.1328,  0.1373,  0.2405,  1.3955]])\n",
            "W2: tensor([[1.3470],\n",
            "        [2.4382],\n",
            "        [0.2028],\n",
            "        [2.4505]])\n",
            "B1: tensor([[ 2.0256,  1.7792, -0.9179, -0.4578]])\n",
            "Salida de la red: tensor([[4.5520]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP9emaIjdJyR"
      },
      "source": [
        "<font color='green'>**Fin Actividad 1**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUGblR3IWGta"
      },
      "source": [
        "Si hizo esto correctamente, debería ver la salida `tensor ([[0.3171]])`.\n",
        "\n",
        "El número de unidades ocultas es un parámetro de la red, a menudo llamado **hiperparámetro** para diferenciarlo de los parámetros de weight y bias. Como verá más adelante cuando analicemos cómo entrenar una red neuronal, cuantas más unidades ocultas tenga una red y más capas, mejor podrá aprender de los datos y hacer predicciones precisas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kBeModZb2Xp",
        "outputId": "737e4e48-e7b3-4034-81f6-70244fb4fe96"
      },
      "source": [
        "import torch\n",
        "n_input = 3\n",
        "n_hidden = 2\n",
        "W1 = torch.randn(n_input, n_hidden)\n",
        "print(W1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2799, -0.9941],\n",
            "        [ 1.8150, -0.6028],\n",
            "        [ 1.6148,  1.9302]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1DNuGbS1i-9it4Nyr3ZMncQz9cRhs2eJr\" width=\"100\" align=\"left\" title=\"Runa-perth\">\n",
        "<br clear=\"left\">\n",
        "\n",
        "## ¿Qué son los tensores?\n",
        "\n",
        "Un tensor es una generalización de números, vectores y matrices. Se puede pensar en un tensor como una matriz multi-dimensional. Los tensores son muy importantes en muchas áreas de la ciencia y la ingeniería, incluyendo la física, la informática, y la inteligencia artificial.\n",
        "\n",
        "Número escalar: Un único número, como 42, es un tensor de 0 dimensiones, también conocido como un escalar.\n",
        "\n",
        "Vector: Una lista de números, como [1, 2, 3], es un tensor de 1 dimensión, también conocido como un vector.\n",
        "\n",
        "Matriz: Una tabla de números, como la siguiente matriz 2x2, es un tensor de 2 dimensiones:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "## Propiedad Universal de los Tensores\n",
        "Un tensor de orden  k se define matemáticamente como un objeto que toma\n",
        "k vectores de entrada y produce un escalar (número) de salida. La propiedad universal se refiere a la capacidad de un tensor de mapear múltiples vectores en un espacio vectorial a un único valor en el campo subyacente (por ejemplo, los números reales).\n",
        "\n",
        "Formalmente, un tensor de orden k se define como un mapeo multilineal\n",
        "\n",
        "$$T: V_1 \\times V_2 \\times \\ldots \\times V_k \\rightarrow F$$\n",
        "\n",
        "En la teoría de categorías, la propiedad universal del producto tensorial se refiere a cómo el producto tensorial de dos objetos (por ejemplo, espacios vectoriales) se define en términos de un objeto y un morfismo que satisfacen ciertas propiedades universales. La propiedad universal se puede utilizar para caracterizar el producto tensorial de manera abstracta y para demostrar su existencia y unicidad.\n",
        "\n",
        "Dada una categoría C con objetos A y B, el producto tensorial A⊗B es un objeto en $C$ junto con un morfismo bilineal $⊗:A×B→A⊗B$  que satisface la siguiente propiedad universal:\n",
        "\n",
        "Para cualquier objeto C en $C$ y cualquier morfismo bilineal  \n",
        "$f:A×B→C$ , existe un único morfismo $g:A⊗B→C$ tal que\n",
        "$f=g∘⊗$.\n",
        "\n",
        "En otras palabras, el producto tensorial\n",
        "A⊗B es el objeto \"más pequeño\" que representa todas las funciones bilineales de  A×B a cualquier otro objeto C en la categoría $C$.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\forall C \\in \\mathcal{C}, \\forall f: A \\times B \\rightarrow C, \\exists! g: A \\otimes B \\rightarrow C \\text{ tal que } f = g \\circ \\otimes\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## ¿Por qué son importantes en las redes neuronales?\n",
        "\n",
        "Los tensores son fundamentales en las redes neuronales por varias razones:\n",
        "\n",
        "Representación de datos: Las redes neuronales suelen trabajar con grandes conjuntos de datos, y los tensores son la estructura de datos ideal para representar estos datos de manera eficiente. Por ejemplo, una imagen a color se puede representar como un tensor de 3 dimensiones, donde las dimensiones corresponden a la altura, la anchura y los canales de color (rojo, verde, azul).\n",
        "\n",
        "Operaciones de alto rendimiento: Las operaciones matemáticas en redes neuronales, como la multiplicación de matrices, se pueden realizar eficientemente en tensores utilizando bibliotecas de álgebra lineal optimizadas, como NumPy en Python o CUDA para GPUs.\n",
        "\n",
        "Diferenciación automática: Las redes neuronales se entrenan utilizando algoritmos de optimización basados en derivadas parciales, y los tensores facilitan el cálculo eficiente de estas derivadas utilizando técnicas de diferenciación automática.\n",
        "\n",
        "Paralelización: Los tensores se prestan naturalmente a la paralelización, lo que significa que las operaciones en tensores se pueden realizar simultáneamente en múltiples núcleos de CPU o GPU. Esto es fundamental para el entrenamiento rápido de redes neuronales en hardware moderno."
      ],
      "metadata": {
        "id": "n7-RsxZk4FUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='purple' style='bold' size=5>**MATERIAL ADICIONAL** </font>"
      ],
      "metadata": {
        "id": "wipggSmdWMDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este video [\"Tensors for Neural Networks, Clearly Explained!!!\"](https://www.youtube.com/watch?v=L35fFDpwIM4) de StatQuest muestra una clara explicación de la utilización de tensores en redes neuronales. Por ejemplo, revisa como estos tensores representan datos, pesos y bias, y cómo se aplican en operaciones como multiplicación de matrices."
      ],
      "metadata": {
        "id": "_bEPuyziWWVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='purple' style='bold'>**FIN MATERIAL ADICIONAL** </font>"
      ],
      "metadata": {
        "id": "ZQTQlXMRWNYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1Igtn9UXg6NGeRWsqh4hefQUjV0hmzlBv\" width=\"100\" align=\"left\" title=\"Runa-perth\">\n",
        "<br clear=\"left\">\n",
        "\n",
        "## Ejercicio Avanzado:\n",
        "\n",
        "Construcción de una Red Neuronal con Dos Capas Ocultas\n",
        "En este ejercicio, construirás y evaluarás una red neuronal con dos capas ocultas utilizando PyTorch. La red deberá tener la siguiente arquitectura:\n",
        "\n",
        "1. Capa de entrada con 3 neuronas.\n",
        "2. Primera capa oculta con 4 neuronas.\n",
        "3. Segunda capa oculta con 3 neuronas.\n",
        "4. Capa de salida con 1 neurona.\n",
        "\n",
        "Las neuronas en las capas ocultas y la capa de salida utilizarán la función de activación sigmoide.\n",
        "\n",
        "**Instrucciones:**\n",
        "\n",
        "1. Utiliza la función torch.randn para generar una matriz de características con dimensiones (1, 3) que represente una única observación con 3 características.\n",
        "\n",
        "2. Inicializa los pesos y los términos de bias para cada capa de la red utilizando la función torch.randn. Asegúrate de que las dimensiones de las matrices de pesos y bias sean coherentes con la arquitectura de la red.\n",
        "\n",
        "3. Utiliza la función de activación sigmoide (torch.sigmoid) y la multiplicación de matrices (torch.mm) para calcular la salida de cada capa.\n",
        "\n",
        "4. Calcula y muestra la salida de la red para la observación generada en el paso 1.\n",
        "\n",
        "**Consejos:**\n",
        "\n",
        "1. Asegúrate de que las dimensiones de las matrices de pesos y bias sean coherentes con la arquitectura de la red.\n",
        "2. Al calcular la salida de cada capa, recuerda sumar el término de bias correspondiente.\n",
        "3. Utiliza la función print para mostrar las matrices de características, pesos, bias y la salida de la red.\n",
        "\n",
        "\n",
        "**Objetivos de aprendizaje:**\n",
        "\n",
        "Familiarizarse con la construcción de redes neuronales más complejas en PyTorch.\n",
        "Practicar la inicialización de pesos y términos de bias en redes neuronales.\n",
        "Entender el cálculo de la salida de una red neuronal con múltiples capas ocultas.\n"
      ],
      "metadata": {
        "id": "A6n9gqiW4GEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. Generar una observación con 3 características\n",
        "torch.manual_seed(26)  # Para reproducibilidad\n",
        "features = torch.randn((1, 3))  # 1 observación, 3 variables\n",
        "print(\"Features:\\n\", features)\n",
        "\n",
        "# 2. Inicializar pesos y bias para cada capa\n",
        "\n",
        "# Capa 1: entrada (3) → oculta 1 (4)\n",
        "W1 = torch.randn((3, 4))\n",
        "B1 = torch.randn((1, 4))\n",
        "\n",
        "# Capa 2: oculta 1 (4) → oculta 2 (3)\n",
        "W2 = torch.randn((4, 3))\n",
        "B2 = torch.randn((1, 3))\n",
        "\n",
        "# Capa 3: oculta 2 (3) → salida (1)\n",
        "W3 = torch.randn((3, 1))\n",
        "B3 = torch.randn((1, 1))\n",
        "\n",
        "print(\"\\nPesos y Bias:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"B1:\", B1)\n",
        "print(\"W2:\", W2)\n",
        "print(\"B2:\", B2)\n",
        "print(\"W3:\", W3)\n",
        "print(\"B3:\", B3)\n",
        "\n",
        "# 3. Forward pass usando sigmoide en todas las capas\n",
        "# Capa oculta 1\n",
        "hidden1 = torch.sigmoid(torch.mm(features, W1) + B1)\n",
        "# Capa oculta 2\n",
        "hidden2 = torch.sigmoid(torch.mm(hidden1, W2) + B2)\n",
        "# Capa de salida\n",
        "output = torch.sigmoid(torch.mm(hidden2, W3) + B3)\n",
        "\n",
        "# 4. Mostrar salida de la red\n",
        "print(\"\\nSalida de la red:\\n\", output)"
      ],
      "metadata": {
        "id": "dgSb6JLD4R8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e070d0a-e561-4cb0-ead1-7980df6389f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features:\n",
            " tensor([[-0.9234, -1.2842, -0.8729]])\n",
            "\n",
            "Pesos y Bias:\n",
            "W1: tensor([[ 0.1461,  1.6910, -1.0566,  0.6336],\n",
            "        [-0.2203, -0.1395, -0.7664,  0.8874],\n",
            "        [ 0.8153,  0.8090,  0.6192, -0.2554]])\n",
            "B1: tensor([[ 0.4567,  0.7805,  0.0319, -0.5938]])\n",
            "W2: tensor([[-0.5724,  0.0422, -0.1804],\n",
            "        [-0.2535,  1.7218, -1.9607],\n",
            "        [ 0.0040, -0.7777, -0.2841],\n",
            "        [ 0.7658,  0.3619, -2.2185]])\n",
            "B2: tensor([[-0.2510,  0.6012,  0.5612]])\n",
            "W3: tensor([[-2.4350],\n",
            "        [ 0.3754],\n",
            "        [-0.5769]])\n",
            "B3: tensor([[2.2237]])\n",
            "\n",
            "Salida de la red:\n",
            " tensor([[0.7846]])\n"
          ]
        }
      ]
    }
  ]
}