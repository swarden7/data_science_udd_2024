{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wx7qk7FbNLRn"},"source":["# **Aprendizaje no supervisado**\n","# UL03. DBSCAN"]},{"cell_type":"markdown","metadata":{"id":"ROx7xWZTSDdW"},"source":["## <font color='blue'>**Algoritmo DBSCAN**</font>\n","\n","DBSCAN corresponde a un algoritmo de aprendizaje de máquina no supervisado, específicamente un algoritmo de **clustering basado en densidad**. Consiste en determinar en qué áreas existen concentraciones de puntos y dónde están separados por áreas vacías o con escasos puntos. Los puntos que no forman parte de un clúster se etiquetan como ruido.\n","\n","Este tipo de algoritmo detecta automáticamente patrones basándose puramente en la ubicación espacial y en la distancia a un número de vecinos especificado.\n","\n","Al contrario de la estrategia seguida por k-means, **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) no presupone clusters convexos, sino que se basa en la densidad de las muestras para identificar los clusters. Por este motivo, los clusters identificados por DBSCAN pueden ser de cualquier forma.\n","\n","El concepto en el que se basa DBSCAN es el de **core points**, o puntos base, que son muestras situadas en áreas de alta densidad. Junto a éstas encontramos también las **border points**, o puntos de borde, que se encuentran próximas a un core point (sin ser una de ellas). Por este motivo, un cluster va a ser una agrupación de core points y de border points situadas a una distancia máxima de alguna core point (distancia medida según algún criterio).\n","\n","### Parámetros\n","\n","* **Eps**: máxima distancia entre dos muestras para poder ser consideradas pertenecientes al mismo \"vecindario\", y\n","* **MinPts**, número de muestras en un vecindario para que un punto pueda ser considerado core point.\n","\n","### Condiciones algoritmo\n","\n","- El Eps-neighborhood (vecindario Eps) de un punto $q$－$N_{Eps}$: Un punto $p \\in N_{Eps}(q)$ si $D(p,q) \\leq Eps$, es decir el punto está dentro del círculo.\n","- Outlier: No pertenece a un grupo.\n","- Core point: $\\left\\vert N_{Eps}(q) \\right\\vert \\geq MinPts$ (densidad de vecindario).\n","- Border point: muestras dentro del cluster, pero que su vecindario no es denso.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1YixvS9xtoOdKzzyaKrItHlqBA3T5S6bn' width=\"500\" align=\"center\" style=\"margin-right: 20px\">\n","\n","- Directamente alcanzable por densidad: Un punto $p$ es **directamente alcanzable por densidad** (*directly density-reachable*) desde un punto $q$ con respecto a $Eps$ y $MinPts$ si:\n","    - $p \\in N_{Eps}(q)$, y $q$ es un **core point**.\n","    - $p$ **no** necesita ser un core point.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1PGycDr6G1up6QcqC6FSm1eAhnwatXfjj' width=\"400\" align=\"center\" style=\"margin-right: 20px\">\n","\n","- Density-reachable: Un punto $p$ es **density-reachable** desde un punto $q$ con respecto a $Eps$ y $MinPts$ si hay una cadena de puntos $p_1, \\dots, p_n,\\ p_1 = q,\\ p_n = p$ tal que $p_{i+1}$ es directamente alcanzable por densidad desde $p_i$.\n","\n","<img src='https://drive.google.com/uc?export=view&id=13HotYfPpIzz0Ywo4sGPlS26Ic9D8P30M' width=\"300\" align=\"center\" style=\"margin-right: 20px\">\n"]},{"cell_type":"markdown","metadata":{"id":"WVOcNA-RFT44"},"source":["## <font color='blue'>**Algoritmo**</font>\n","\n","1. Elegir aleatoriamente un punto $p$.\n","2. Identificar todos los puntos alcanzables por densidad desde $p$ con respecto a $Eps$ y $MinPts$.\n","3. Si $p$ es un core point, un cluster es formado.\n","4. Si $p$ es un border point, ningún punto es de densidad alcanzable desde $p$, luego continuar con un siguiente punto.\n","5. Repetir el proceso hasta que se hayan procesado todos los puntos.\n","\n","Una buena forma de entender el algoritmo es visualizar el proceso. En el siguiente link podrán ver una visualización del funcionamiento de DBSCAN para distintos conjuntos de datos:\n","\n","https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"92L6RfVkKsQx"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import seaborn as sns; sns.set()  # for plot styling\n","import numpy as np\n","import warnings\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bvx4nhXAKlr1"},"source":["# Librerías necesarias\n","from IPython.display import display\n","from sklearn import metrics\n","from sklearn.datasets import make_blobs, make_circles\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import DBSCAN\n","\n","warnings.filterwarnings('ignore')\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"96OMg84FKKVC"},"source":["Aplicaremos DBSCAN sobre un conjunto de datos esféricos (similar al usado en k-means)."]},{"cell_type":"code","metadata":{"id":"mO_e0WL_KRcO"},"source":["# Generación de la data\n","X, y = make_blobs(n_samples=1000,\n","                  n_features=2,\n","                  centers=3,\n","                  random_state=170)\n","\n","# Normalización de las variables\n","X = StandardScaler().fit_transform(X)\n","\n","# Entrenar DBSCAN\n","y_pred = DBSCAN(eps=0.3, min_samples=30).fit_predict(X)\n","\n","# Graficar las predicciones\n","plt.scatter(X[:,0], X[:,1], c=y_pred)\n","\n","# Imprimir los resultados\n","print('Número de clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n","print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n","print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n","print('Mean Silhouette score: {}'.format(metrics.silhouette_score(X, y_pred)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Ag8xazpLvOg"},"source":["Los **puntos negros** corresponden a los **outliers** para el resultado anterior.\n","\n","Tenga en cuenta que **no es necesario especificar el número de clústeres** con el algoritmo DBSCAN. Además, DBSCAN es bueno para descubrir los valores atípicos sin requerir algunos trucos como lo hicimos anteriormente para el algoritmo de k-means."]},{"cell_type":"markdown","metadata":{"id":"YTTjmRMLMgPb"},"source":["Sin embargo DBSCAN no siempre funciona bien, por ejemplo aumentemos el número de puntos que definen nuestro conjunto de prueba (hasta 5000 como el usado para el ejemplo de k-means)."]},{"cell_type":"code","metadata":{"id":"DyEWd0eSMd0N"},"source":["# Generación de la data\n","X, y = make_blobs(n_samples=5000,\n","                  n_features=2,\n","                  centers=3,\n","                  random_state=170)\n","\n","# Normalización de las variables\n","X = StandardScaler().fit_transform(X)\n","\n","# Entrenar DBSCAN\n","y_pred = DBSCAN(eps=0.3, min_samples=30).fit_predict(X)\n","\n","# Graficar las predicciones\n","plt.scatter(X[:,0], X[:,1], c=y_pred)\n","\n","# Imprimir los resultados\n","print('Número de clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n","print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n","print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n","print('Mean Silhouette score: {}'.format(metrics.silhouette_score(X, y_pred)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ScwS8E_tMu91"},"source":["Podemos observar que DBSCAN solo logra determinar (en esta configuración de hiperparámetros del modelo) solo 2 clusters de los 3 que intuitivamente somos capaces de determinar.\n","\n","Para solucionar este problema debemos trabajar con los hiperparámetros del modelo, en este caso **eps** y **min_samples** (MinPts)."]},{"cell_type":"code","metadata":{"id":"g_4wCf_mNdZI"},"source":["# Generación de la data\n","X, y = make_blobs(n_samples=5000,\n","                  n_features=2,\n","                  centers=3,\n","                  random_state=170)\n","\n","# Normalización de las variables\n","X = StandardScaler().fit_transform(X)\n","\n","# Entrenar DBSCAN\n","y_pred = DBSCAN(eps=0.12, min_samples=5).fit_predict(X)\n","\n","# Graficar las predicciones\n","plt.scatter(X[:,0], X[:,1], c=y_pred)\n","\n","# Imprimir los resultados\n","print('Número de clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n","print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n","print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n","print('Mean Silhouette score: {}'.format(metrics.silhouette_score(X, y_pred)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzUa9q2DNnVz"},"source":["Con esta configuración de hiperparámetros es posible encontrar un resultado más cercano, en el cual es posible identificar los 3 clusters."]},{"cell_type":"markdown","metadata":{"id":"8VRysSkIPvry"},"source":["Otro ejemplo: Probemos DBSCAN con el dataset lunas que tantos problemas genera para el algoritmo k-Means."]},{"cell_type":"code","metadata":{"id":"5lNC_-SNP5zI"},"source":["from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=400, noise=0.05, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"unM3HmWKbRLm"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dw7QQiPXP9E1"},"source":["plt.scatter(X[:,0], X[:,1])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fovY5tjXQAI0"},"source":["model = DBSCAN(eps = 0.2)\n","clusters = model.fit_predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrZUdvECQOId"},"source":["plt.scatter(X[:,0], X[:,1], c=clusters)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQnxloAlOXfs"},"source":["## <font color='green'>Actividad 1</font>\n","\n","Considere el siguiente conjunto de datos no esféricos:\n","\n","```\n","# Generación de data no esférica.\n","X, y = make_circles(n_samples=1000, factor=0.3, noise=0.1)\n","```\n","\n","1. Implemente un modelo DBSCAN para agrupar sus datos (recuerde los pasos metoddológicos necesarios para implementar un buen modelo). Calcule las siguientes métricas: i) número de clusters, ii) homogeneidad, iii) completitud y iv) mean silhouette score.\n","\n","2. Repita el proceso implementando un algoritmo k-means.\n","\n","3. Grafique la salida de ambos modelos y comente sus resultados.\n","\n"]},{"cell_type":"code","metadata":{"id":"4qUippWEPYHc"},"source":["# Tu código aquí ...\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VeOlPDshPYgF"},"source":["<font color='green'>Fin Actividad 1</font>\n"]},{"cell_type":"markdown","metadata":{"id":"nsSMtq6wRScZ"},"source":["## <font color='blue'>**Parámetros DBSCAN**</font>\n","\n","DBSCAN tiene algunos parámetros y de ellos, dos son cruciales: eps y MinPts.\n","\n","Eps es la distancia más lejana a la que un punto seleccionará a sus vecinos. MinPts se refiere al número de puntos vecinos necesarios para que un punto se considere una región densa o un grupo válido. Por lo general, MinPts se establece en un valor que tenga sentido para el conjunto de datos y el número de dimensiones presentes. Esto determinará el número de valores atípicos identificados. Sin embargo, este parámetro no es tan crucial como eps.\n","\n","**El parámetro más importante de DBSCAN es eps**. Como indicamos previamente es la distancia más lejana a la que un punto seleccionará a sus vecinos. Por tanto, esto decidirá intuitivamente cuántos vecinos descubrirá un punto. Esto dependerá de la distribución de los datos en sí.\n"]},{"cell_type":"code","metadata":{"id":"mb5HhgUwQifO"},"source":["centers = [[1, 0.5], [2, 2], [1, -1]]\n","stds = [0.1, 0.4, 0.3]\n","X, labels_true = make_blobs(n_samples=1000, centers=centers, cluster_std=stds, random_state=0)\n","fig = plt.figure(figsize=(10, 10))\n","sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels_true)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLdipCxtVfce"},"source":["Hagamos DBSCAN con algunos valores para nuestro conjunto de datos."]},{"cell_type":"code","metadata":{"id":"yD-ya3sdUd2T"},"source":["db = DBSCAN(eps=0.5, min_samples=10).fit(X)\n","labels = db.labels_\n","fig = plt.figure(figsize=(10, 10))\n","sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WicFXe9FVmvU"},"source":["Podemos ver claramente en nuestra última figura que dos grupos se han fusionado.Estas situaciones pueden producir problemas en una aplicación de agrupación en problemas reales.\n","\n","Variaremos el valor de eps para ver su efecto sobre los clusters."]},{"cell_type":"code","metadata":{"id":"ZsXaxVVFWCyp"},"source":["fig = plt.figure(figsize=(20, 10))\n","fig.subplots_adjust(hspace=.5, wspace=.2)\n","i = 1\n","for x in range(10, 0, -1):\n","    eps = 1/(11-x)\n","    db = DBSCAN(eps=eps, min_samples=10).fit(X)\n","    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n","    core_samples_mask[db.core_sample_indices_] = True\n","    labels = db.labels_\n","\n","    ax = fig.add_subplot(2, 5, i)\n","    ax.set_title(f\"eps = {round(eps, 2)}\", fontsize=15)\n","    sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels)\n","    i += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJJ5nrRdWtjN"},"source":["Podemos ver que alcanzamos un punto óptimo entre eps = 0.1 y eps = 0.33. Los valores de eps más pequeños tienen demasiado ruido o valores atípicos (se muestran en color verde)."]},{"cell_type":"markdown","metadata":{"id":"FyaB3fCIXOWR"},"source":["## <font color='blue'>**Método para el tuning del parámetro eps**</font>\n","\n","Dado que el valor de eps es proporcional al número esperado de vecinos descubiertos, podemos usar los vecinos más cercanos (KNN) para encontrar una estimación buena para el eps. Calculemos los vecinos más cercanos."]},{"cell_type":"code","metadata":{"id":"r7W8KSw4Xo0h"},"source":["from sklearn.neighbors import NearestNeighbors\n","nearest_neighbors = NearestNeighbors(n_neighbors=11)\n","neighbors = nearest_neighbors.fit(X)\n","distances, indices = neighbors.kneighbors(X)\n","distances = np.sort(distances[:,10], axis=0)\n","fig = plt.figure(figsize=(5, 5))\n","plt.plot(distances)\n","plt.xlabel(\"Puntos\")\n","plt.ylabel(\"Distancia\")\n","plt.savefig(\"Distance_curve.png\", dpi=300)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HUH8YJm-XyRa"},"source":["Note que en el cálculo del vecino más cercano, el punto en sí aparecerá como el primer vecino más cercano. Entonces buscamos los 11 vecinos más cercanos. Ordenamos la distancia al décimo vecino más cercano y graficamos la variación de la distancia. Como podemos ver, el **punto de codo** aparece en algún lugar entre 0,1 y 0,3. Muy parecido al resultados que encontramos gráficamente al explorar los datos."]},{"cell_type":"markdown","metadata":{"id":"cC8if-WmYcrV"},"source":["Ville Satopaa et al. presentaron el paper “Finding a “Kneedle” in a Haystack: Detecting Knee Points in System Behavior” en el año 2011. En este artículo, se presenta una forma de detectar el punto de codo (elbow point)."]},{"cell_type":"code","metadata":{"id":"94lAKGJ-ZOGr"},"source":["pip install kneed"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from kneed import KneeLocator\n","import matplotlib.pyplot as plt\n","\n","# Calcular el \"knee point\"\n","i = np.arange(len(distances))\n","knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n","print(distances[knee.knee])\n","\n","# Crear la figura y graficar el codo en un solo paso\n","plt.figure(figsize=(5, 5))\n","plt.plot(i, distances, label='Distancias')\n","plt.scatter(knee.knee, distances[knee.knee], color='red', label='Knee Point', zorder=5)\n","plt.xlabel(\"Puntos\")\n","plt.ylabel(\"Distancia\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"vn3-tGUl_8bZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TIb2omuZUr_"},"source":["Podemos ver que el punto de inflexión detectado por este método tiene un valor de 0.187. Ahora podemos usar este valor como nuestro eps para ver cómo se vería nuestro nuevo agrupamiento."]},{"cell_type":"code","metadata":{"id":"OEDcmHDhZg2S"},"source":["db = DBSCAN(eps=0.187, min_samples=10).fit(X)\n","labels = db.labels_\n","fig = plt.figure(figsize=(10, 10))\n","sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYfoCK2MZqog"},"source":["Podemos ver que tenemos una estimación razonable de la agrupación real. Esto suele ser suficiente para el trabajo de investigación. Si la inexistencia de valores atípicos es una suposición intuitiva para el escenario, uno puede simplemente usar los vecinos más cercanos calculados para reasignar los puntos atípicos (denominados como grupo - 1) a los grupos detectados."]},{"cell_type":"markdown","metadata":{"id":"iTP9cqZjZ1gl"},"source":["Hay algunas suposiciones implícitas en este enfoque:\n","\n","* Las densidades en todos los grupos son las mismas.\n","* Los tamaños de los conglomerados o las desviaciones estándar son los mismos.\n","\n","Estas suposiciones están implícitas cuando consideramos el mismo nivel de vecino para el cálculo del punto de codo. Sin embargo, en los datos originales, podemos ver claramente que las densidades no son las mismas. Esta es la razón principal por la que observamos algunos valores atípicos a pesar de que los puntos se distribuyen utilizando una desviación estándar fija."]},{"cell_type":"markdown","metadata":{"id":"zJSRe_UabZQT"},"source":["## <font color='green'>Actividad 2</font>\n","\n","A continuación se presenta un código que permite graficar una muestra de datos 2D generados a partir de una distribución uniforme.\n","\n","El código además permite explorar en forma interactiva los parámetros eps y min_samples (MinPts) del algoritmo DBSCAN.\n","\n","```\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('ggplot')\n","import numpy as np\n","from ipywidgets import interact, widgets\n","from sklearn.metrics.pairwise import euclidean_distances\n","from sklearn.cluster import DBSCAN\n","\n","# Creamos datos 2D ficticios tomando muestras de una distribución uniforme  \n","data_matr = np.random.rand(200, 2)\n","\n","# Calcular los márgenes para plotear los resultados\n","x_min, y_min = data_matr.min(axis=0)\n","x_max, y_max = data_matr.max(axis=0)\n","x_margin = (x_max - x_min) * 0.1\n","y_margin = (y_max - y_min) * 0.1\n","\n","def render_glyphs(data_matr, int_glyphs, ax):\n","    \"\"\"Crear un scatter plot dada una matriz 2D y matplotlib.Axes.\"\"\"\n","    for (x, y), g in zip(data_matr, int_glyphs):\n","        is_noise = g < 0\n","        ax.text(x, y, str(g), alpha=0.5 if is_noise else 1.0)\n","\n","def update_dbscan_plot(eps, min_pts):\n","    \"\"\"Cluster `data_matr` y generar una gráfica de las etiquetas del cluster.\"\"\"\n","    \n","    # DBSCAN\n","    dist_matr = euclidean_distances(data_matr)\n","    dbscan = DBSCAN(eps=eps, min_samples=min_pts, metric='precomputed',\n","               algorithm='auto', leaf_size=30, p=None, n_jobs=1)\n","    dbscan.fit(dist_matr)\n","    \n","    # Generar gráficos\n","    fig, ax = plt.subplots()\n","    render_glyphs(data_matr, dbscan.labels_, ax)\n","    ax.set_xlim(x_min - x_margin, x_max + x_margin)\n","    ax.set_ylim(y_min - y_margin, y_max + y_margin)\n","    ax.set_xlabel('$x$')\n","    ax.set_ylabel('$y$')\n","    ax.set_title('Cluster number')\n","\n","# Exploración interactiva de DBSCAN\n","interact(update_dbscan_plot,\n","         eps=widgets.FloatSlider(value=0.03, min=1e-10, max=0.4, step=0.005, readout_format='.3f'),\n","         min_pts=widgets.IntSlider(value=3, min=2, max=15, step=1, readout_format='d'));\n","```\n","1. Interctúe con los parámetros del modelo y estime cuál sería el mejor seteo de hiperparámetros propuestos para esta muestra de datos.\n","\n","2. Utilice el método de Ville Satopaa et al. para encontrar el punto de codo para obtener los mejores resultados de distribución para esta muestra de datos. Grafique sus resultados y compare con los resultados que encontró en el punto 1 al realizar el análisis visual."]},{"cell_type":"code","metadata":{"id":"-gNipGdNcihw"},"source":["# Tu código aquí ...\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rRQIT4LENlqY"},"source":["<font color='green'>Fin Actividad 2</font>\n"]},{"cell_type":"markdown","metadata":{"id":"vpTEcTQjW-ol"},"source":["## <font color='green'>Actividad 3</font>\n","\n","El archivo multishape.csv contiene una muestra de datos. Aplicar el algoritmo DBSCAN sobre los datos del archivo.\n","\n","1. Calcular las métricas de calidad de clusters: de homogeneidad, completitud, y mean silhouette score.\n","\n","2. Seleccionar los mejores parámetros (eps, min_sample) según los puntajes de calidad del clúster.\n","\n","3. Utilice el algoritmo de punto codo para encontrar sus hiperparámetros. Compare con los resultados del punto 3.\n"]},{"cell_type":"code","metadata":{"id":"Q6X1pd4MZaIa"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YgMs0qxgP_Xh"},"source":["df = pd.read_csv('/content/drive/MyDrive/Becas_Capital_Humano_24/Material_Clases/M05-Apredizaje_No_supervisado/data/UL03.csv')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fSbjVYf5Qdsq"},"source":["# Tu código aquí ...\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1u7K4W2kZaSt"},"source":["<font color='green'>Fin Actividad 3</font>\n"]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=view&id=1bqkJJ7QiIOTsp-7jm5eUtK-XYWMEq2_K\" width=\"100\" align=\"left\" />\n","<br clear=\"left\">\n","\n","## <font color='blue'>**Resumen**</font>\n","\n","DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de agrupamiento no supervisado que se basa en la densidad de los puntos en un conjunto de datos. Agrupa puntos cercanos que tienen suficientes vecinos cercanos dentro de un radio específico (épsilon) como puntos núcleo y expande el clúster al agrupar puntos alcanzables desde esos puntos núcleo. Los puntos que no están dentro de ningún clúster y no tienen suficientes vecinos cercanos se consideran puntos de ruido.\n","\n","Ventajas de DBSCAN:\n","\n","1. No requiere especificar el número de clústeres de antemano, lo que lo hace útil para conjuntos de datos con una cantidad desconocida de clústeres.\n","\n","2. Puede identificar clústeres de diferentes formas y tamaños, ya que no asume formas específicas para los clústeres.\n","\n","3. Robusto ante valores atípicos, ya que los puntos de ruido se identifican automáticamente y no se agrupan en ningún clúster.\n","\n","4. No se ve afectado por la elección de centroides iniciales, ya que utiliza una estrategia basada en la densidad para formar los clústeres.\n","\n","Desventajas de DBSCAN:\n","\n","1. Sensible a la elección de los parámetros épsilon y minPts, que pueden afectar significativamente los resultados del agrupamiento.\n","\n","2. Puede tener dificultades con conjuntos de datos de alta dimensionalidad debido al problema de la maldición de la dimensionalidad.\n","\n","Ejemplos de aplicaciones de DBSCAN:\n","\n","1. Segmentación de datos espaciales: DBSCAN es ampliamente utilizado en datos geoespaciales, como la identificación de grupos de puntos cercanos en mapas.\n","\n","2. Detección de anomalías: DBSCAN puede detectar valores atípicos o puntos ruidosos en conjuntos de datos.\n","\n","3. Agrupamiento de datos en minería de texto: Puede agrupar documentos o texto similar basado en la similitud de contenido.\n","\n","4. Clasificación de imágenes: Puede ser utilizado para agrupar imágenes similares basadas en sus características.\n","\n","5. Reconocimiento de patrones en bases de datos biológicas o médicas.\n","\n","En resumen, DBSCAN es un algoritmo de agrupamiento efectivo que se adapta bien a diversos tipos de datos y tiene aplicaciones en diversas áreas, especialmente en problemas de agrupamiento espacial y detección de anomalías. Sin embargo, se debe tener cuidado al seleccionar los parámetros adecuados para obtener resultados óptimos.\n"],"metadata":{"id":"7Vm-AJd-M1DG"}}]}