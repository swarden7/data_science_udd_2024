{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQPlt8gU5H1m"
      },
      "source": [
        "# Deep Learning\n",
        "# DL00 Motivacion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cytwv_g7hOf"
      },
      "source": [
        "## <font color='blue'>**Cosas interesantes que han logrado las redes neuronales**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUyR7eRr5TFk"
      },
      "source": [
        "### Las redes neuronales profundas han superados a los mejores jugadores de Go y Starcraft.\n",
        "\n",
        "\n",
        "![Deteccón](https://drive.google.com/uc?export=view&id=1m-Cbha6KvjAVxksFPV-6PywIEuGbLm6f)\n",
        "\n",
        "\n",
        "1. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484-489.\n",
        "\n",
        "2. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., ... & Silver, D. (2019). Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782), 350-354."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FcFRogQz6ml"
      },
      "source": [
        "### El auto de conducción autonoma.\n",
        "El aprendizaje profundo es la fuerza que está dando vida a la conducción autónoma. Gran cantidad de datos se envían a un sistema para construir un modelo, entrenar a las máquinas para que aprendan y luego probar los resultados en un entorno seguro. La principal preocupación de los desarrolladores de automóviles autónomos es manejar escenarios sin precedentes. Un ciclo regular de pruebas e implementación típico de los algoritmos de aprendizaje profundo garantiza una conducción segura con una exposición cada vez mayor a millones de escenarios. Los datos de cámaras, sensores y mapas geográficos ayudan a crear modelos sofisticados para navegar a través del tráfico, identificar caminos, señalización, rutas solo para peatones y elementos en tiempo real como el volumen del tráfico y los bloqueos de carreteras. Según Forbes, el MIT está desarrollando un nuevo sistema que permitirá a los automóviles autónomos navegar sin un mapa, ya que el mapeo en 3-D aún se limita a las áreas principales del mundo y no es tan efectivo para evitar contratiempos. \"La razón por la que este tipo de enfoque\" sin mapas \"no se ha hecho antes es porque, en general, es mucho más difícil alcanzar la misma precisión y confiabilidad que con mapas detallados. Un sistema como este, que puede navegar solo con sensores a bordo, muestra el potencial de que los autos autónomos sean capaces de manejar carreteras más allá del pequeño número que las empresas de tecnología han mapeado \".\n",
        "\n",
        "![Deteccón](https://drive.google.com/uc?export=view&id=1Q5kFIRIBxKJjJdnPqqEa5ClcJiXkAB4O)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoYDPRnZ5NJi"
      },
      "source": [
        "### En procesamiento de lenguaje natural\n",
        "\n",
        "Comprender las complejidades asociadas con el lenguaje, ya sea sintaxis, semántica, matices tonales, expresiones o incluso sarcasmo, es una de las tareas más difíciles de aprender para los humanos. El entrenamiento constante desde el nacimiento y la exposición a diferentes entornos sociales ayudan a los humanos a desarrollar respuestas adecuadas y una forma de expresión personalizada para cada escenario. El procesamiento del lenguaje natural a través del aprendizaje profundo está tratando de lograr lo mismo entrenando máquinas para captar los matices lingüísticos y enmarcar las respuestas adecuadas. Por ejemplo\n",
        "\n",
        "El resumen de documentos se está utilizando y probando ampliamente en el ámbito legal, lo que hace que los asistentes legales sean obsoletos.\n",
        "\n",
        "Responder preguntas, modelar el lenguaje, clasificar texto, análisis de Twitter o análisis de sentimientos en un nivel más amplio son todos subconjuntos del procesamiento del lenguaje natural donde el aprendizaje profundo está ganando impulso.\n",
        "\n",
        "El problema es predecir la siguiente palabra dadas las palabras anteriores. La tarea es fundamental para el reconocimiento óptico de caracteres o el habla, y también se utiliza para la corrección ortográfica, el reconocimiento de escritura a mano y la traducción automática estadística.\n",
        "\n",
        "[Generative Model-Based Text-to-speech Synthesis](https://github.com/oxford-cs-deepnlp-2017/lectures/blob/master/Lecture%2010%20-%20Text%20to%20Speech.pdf)\n",
        "\n",
        "Sistemas de respuesta a preguntas que intentan responder la consulta de un usuario que se formula en forma de pregunta devolviendo la frase none adecuada, como una ubicación, una persona o una fecha.\n",
        "\n",
        "[How To Build a Question Answering Bot](https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrJoI23X2KtF"
      },
      "source": [
        "### Asistentes virtuales\n",
        "\n",
        "Una de las aplicaciones más populares de aprendizaje profundo son los asistentes virtuales que van desde Alexa hasta Siri y el Asistente de Google. Cada interacción con estos asistentes les brinda la oportunidad de aprender más sobre su voz y acento, brindándole así una experiencia secundaria de interacción humana. Los asistentes virtuales utilizan el aprendizaje profundo para saber más sobre sus temas, desde sus preferencias para salir a cenar hasta los lugares más visitados o sus canciones favoritas. Aprenden a comprender sus comandos evaluando el lenguaje humano natural para ejecutarlos. Otra capacidad con la que están dotados los asistentes virtuales es traducir su discurso a texto, tomar notas para usted y reservar citas. Los asistentes virtuales están literalmente a su disposición, ya que pueden hacer de todo, desde hacer mandados hasta responder automáticamente a sus llamadas específicas y coordinar tareas entre usted y los miembros de su equipo. Con aplicaciones de aprendizaje profundo, como generación de texto y resúmenes de documentos, los asistentes virtuales también pueden ayudarlo a crear o enviar una copia de correo electrónico adecuada.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10It5Uh5mKkOQEwsKa_R4M8fwqpTFd7Gx\" width='700'>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GH9n-b_5iMr"
      },
      "source": [
        "### En procesamiento de imagenes\n",
        "\n",
        "La detección de objetos es la tarea de la clasificación de imágenes con localización, aunque una imagen puede contener múltiples objetos que requieren localización y clasificación.\n",
        "\n",
        "\n",
        "![Deteccón](https://drive.google.com/uc?export=view&id=12BRJQlZggZiAyHobuN-e-Zv1b1VEAAId)\n",
        "\n",
        "\n",
        "La segmentación de objetos, o segmentación semántica, es la tarea de detección de objetos donde se dibuja una línea alrededor de cada objeto detectado en la imagen. La segmentación de imágenes es un problema más general de dividir una imagen en segmentos.\n",
        "\n",
        "![Segmentacion](https://drive.google.com/uc?export=view&id=1Dmy4OOl4MBrO1w4TgTUgP3WxnpLk4i4g)\n",
        "\n",
        "\n",
        "La transferencia de estilo o transferencia de estilo neuronal es la tarea de aprender el estilo de una o más imágenes y aplicar ese estilo a una nueva imagen.\n",
        "\n",
        "![styleTransfer](https://drive.google.com/uc?export=view&id=1jT6e5p6vtnBrwTzb63qCsIQuz_lh1IYa)\n",
        "\n",
        "Proyectos en Chile\n",
        "\n",
        "[Alba Prototipo 1](https://youtu.be/mDakeoaIB3o)\n",
        "\n",
        "[Alba Prototipo 4](https://youtu.be/YEiFeQFh2uo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za68MT8K3aU8"
      },
      "source": [
        "### Healthcare\n",
        "\n",
        "Según NVIDIA, “desde la obtención de imágenes médicas hasta el análisis de genomas y el descubrimiento de nuevos fármacos, toda la industria de la salud se encuentra en un estado de transformación y la computación GPU está en el centro. Las aplicaciones y los sistemas acelerados por GPU brindan nuevas eficiencias y posibilidades, lo que capacita a los médicos, clínicos e investigadores apasionados por mejorar la vida de los demás para que hagan su mejor trabajo \". Ayudar al diagnóstico temprano, preciso y rápido de enfermedades potencialmente mortales, médicos aumentados que abordan la escasez de médicos y proveedores de atención médica de calidad, resultados de patología y estandarización del curso de tratamiento, y comprensión de la genética para predecir el riesgo futuro de enfermedades y episodios de salud negativos son algunos de los aspectos más importantes. Los proyectos de aprendizaje se aceleran en el ámbito de la salud. Sin embagro, el escepticismo de los médicos y la falta de un enorme conjunto de datos todavía plantean desafíos para el uso del aprendizaje profundo en medicina.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=15XZVXzU3tJKpWSrR6XcG5LurQOfNUNGZ\" width='700'>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTlZsB7_4z_Z"
      },
      "source": [
        "### Adición de sonidos a películas mudas\n",
        "\n",
        "Una aplicación tanto de redes neuronales convolucionales como de redes neuronales recurrentes LSTM implica sintetizar sonidos para que coincidan con videos silenciosos. Un modelo de aprendizaje profundo tiende a asociar los fotogramas de video con una base de datos de sonidos pregrabados para seleccionar los sonidos apropiados para la escena. Esta tarea se realiza usando videos de entrenamiento 1000, que tienen baquetas que suenan impactantes en diferentes superficies y crean diferentes sonidos. Estos videos luego son utilizados por modelos de aprendizaje profundo para predecir el sonido más adecuado en el video. Y luego, para predecir si el sonido es falso o real, se crea una configuración similar a una prueba de Turing para lograr los mejores resultados.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1U8-8NipsLGqaOZRQy9l-VbMgqle71--Q\" width='700'>\n",
        "<br>\n",
        "\n",
        "\n",
        "El video completo [aquí](https://youtu.be/0FW99AQmMc8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KZP2lWaUum8"
      },
      "source": [
        "## <font color='blue'>**Historia de las redes neuronales**</font>\n",
        "\n",
        "\n",
        "La idea de las redes neuronales comenzó como un modelo de cómo funcionan las neuronas en el cerebro, denominado \"conexionismo\" y utilizaba circuitos conectados para simular el comportamiento inteligente. En 1943, el neurofisiólogo __Warren McCulloch__ y el matemático __Walter Pitts__, crearon un modelo informático de una red neuronal.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1wZ1nbYP-kMt6yxSt-8905IOtu45uc54u\" width='700'>\n",
        "<br>  \n",
        "\n",
        "\n",
        "__Donald Hebb__ llevó la idea más allá en su libro, The Organization of Behavior (1949), proponiendo que las vías neuronales se fortalecen con cada uso sucesivo, especialmente entre neuronas que tienden a dispararse al mismo tiempo, iniciando así el largo viaje hacia la cuantificación de los complejos procesos de el cerebro. A este mecansmo se le conoce como el __Aprendizaje de Hebb__.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1aOOY5hKy3r0_Fnlf3dYt8DlgTyTiPdE1\" width='600'>\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA5j9E6WXm31"
      },
      "source": [
        "### Dos conceptos fundamentales\n",
        "\n",
        "1. __Threshold Logic__: conversión de una entrada continua,  en una salida discreta.\n",
        "2. __Hebbian Learning__: un modelo de aprendizaje basado en la plasticidad neuronal, propuesto por Donald Hebb en su libro \"La organización del comportamiento\", a menudo resumido con la frase: \"Células que se activan juntas, se conectan juntas\".\n",
        "\n",
        "Estos conceptos propuestos en la década de 1940, se trataron de llevar a la practica en la década de 1950, cuando los investigadores comenzaron a intentar traducir estas redes en sistemas computacionales, la primera red Hebbian se implementó con éxito en el MIT en 1954.\n",
        "\n",
        "Por esta época, __Frank Rosenblatt__, psicólogo de Cornell, estaba trabajando para comprender los sistemas de decisión comparativamente más simples presentes en el ojo de una mosca, que subyacen y determinan su respuesta de huida. En un intento por comprender y cuantificar este proceso, propuso la idea de un perceptrón en 1958, llamándolo __Perceptrón Mark I__. Era un sistema con una relación entrada-salida simple, modelado en una neurona McCulloch-Pitts. Una neurona de McCulloch-Pitts toma entradas, toma una suma ponderada y devuelve \"0\" si el resultado está por debajo del umbral y \"1\" en caso contrario."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DECONSTRUCCIÓN\n",
        "conjunto_de_entrenamiento = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]\n",
        "for vector_de_entrada, salida_deseada in conjunto_de_entrenamiento:\n",
        "    print(vector_de_entrada)\n",
        "    print(salida_deseada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KaqGDeg-mpd",
        "outputId": "38658b8a-ac7f-40fe-8e11-4cd123c71228"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 0, 0)\n",
            "1\n",
            "(1, 0, 1)\n",
            "1\n",
            "(1, 1, 0)\n",
            "1\n",
            "(1, 1, 1)\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c2OzL-9ZA6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a020bf-8bc3-4284-b9d6-9ace73c0ffea"
      },
      "source": [
        "# El perceptron de Rosenblatt Obtenido de Wikipedia\n",
        "umbral = 0.5\n",
        "tasa_de_aprendizaje = 0.1\n",
        "pesos = [0, 0, 0]\n",
        "conjunto_de_entrenamiento = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]\n",
        "\n",
        "def producto_punto(valores, pesos):\n",
        "    return sum(valor * peso for valor, peso in zip(valores, pesos))\n",
        "\n",
        "while True:\n",
        "    print('-' * 60)\n",
        "    contador_de_errores = 0\n",
        "    for vector_de_entrada, salida_deseada in conjunto_de_entrenamiento:\n",
        "        print(pesos)\n",
        "        resultado = producto_punto(vector_de_entrada, pesos) > umbral\n",
        "        error = salida_deseada - resultado\n",
        "        if error != 0:\n",
        "            contador_de_errores += 1\n",
        "            for indice, valor in enumerate(vector_de_entrada):\n",
        "                pesos[indice] += tasa_de_aprendizaje * error * valor\n",
        "    if contador_de_errores == 0:\n",
        "        break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "[0, 0, 0]\n",
            "[0.1, 0.0, 0.0]\n",
            "[0.2, 0.0, 0.1]\n",
            "[0.30000000000000004, 0.1, 0.1]\n",
            "------------------------------------------------------------\n",
            "[0.30000000000000004, 0.1, 0.1]\n",
            "[0.4, 0.1, 0.1]\n",
            "[0.5, 0.1, 0.2]\n",
            "[0.5, 0.1, 0.2]\n",
            "------------------------------------------------------------\n",
            "[0.4, 0.0, 0.1]\n",
            "[0.5, 0.0, 0.1]\n",
            "[0.5, 0.0, 0.1]\n",
            "[0.6, 0.1, 0.1]\n",
            "------------------------------------------------------------\n",
            "[0.5, 0.0, 0.0]\n",
            "[0.6, 0.0, 0.0]\n",
            "[0.6, 0.0, 0.0]\n",
            "[0.6, 0.0, 0.0]\n",
            "------------------------------------------------------------\n",
            "[0.5, -0.1, -0.1]\n",
            "[0.6, -0.1, -0.1]\n",
            "[0.7, -0.1, 0.0]\n",
            "[0.7, -0.1, 0.0]\n",
            "------------------------------------------------------------\n",
            "[0.6, -0.2, -0.1]\n",
            "[0.6, -0.2, -0.1]\n",
            "[0.7, -0.2, 0.0]\n",
            "[0.7999999999999999, -0.1, 0.0]\n",
            "------------------------------------------------------------\n",
            "[0.7, -0.2, -0.1]\n",
            "[0.7, -0.2, -0.1]\n",
            "[0.7, -0.2, -0.1]\n",
            "[0.7999999999999999, -0.1, -0.1]\n",
            "------------------------------------------------------------\n",
            "[0.7, -0.2, -0.2]\n",
            "[0.7, -0.2, -0.2]\n",
            "[0.7999999999999999, -0.2, -0.1]\n",
            "[0.7999999999999999, -0.2, -0.1]\n",
            "------------------------------------------------------------\n",
            "[0.7999999999999999, -0.2, -0.1]\n",
            "[0.7999999999999999, -0.2, -0.1]\n",
            "[0.7999999999999999, -0.2, -0.1]\n",
            "[0.7999999999999999, -0.2, -0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV_FXsyobeDc"
      },
      "source": [
        "La belleza de Mark I Perceptron radica en el hecho de que sus pesos se \"aprenden\" a través de entradas pasadas sucesivamente, mientras se minimiza la diferencia entre la salida deseada y la real.\n",
        "\n",
        "¿Un gran inconveniente? Este perceptrón solo pudo aprender a separar clases linealmente separables, haciendo que el circuito o exclusivo, simple pero no lineal, sea una barrera infranqueable.\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1tA7ByRmhDnwjncXCN889h_f4uRsMQEk-\" width='700'>\n",
        "<br>\n",
        "\n",
        "A pesar del  desordenado  uso del aprendizaje automático para cuantificar los sistemas de decisión además del cerebro, las redes neuronales artificiales de hoy no son más que varias capas de estos perceptrones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFo8R4pqXd1E"
      },
      "source": [
        "Luego hubo un invierno el cual comenzó a finalizar en 1982 en la Academia Nacional de Ciencias cuando __Jon Hopfield__ presentó su artículo sobre lo que llegó a conocerse como __Hopfield Net__, mientras que el mismo año en la conferencia EE.UU.-Japón sobre Redes Neuronales Cooperativas / Competitivas, Japón anunció su intención de comenzar su esfuerzo de quinta generación en redes neuronales. Esto hizo que los fondos comenzaran a fluir nuevamente desde las arcas de una nación que temía quedarse atrás. Pronto, el Instituto Americano de Física, en 1985, estableció una reunión anual de \"Redes neuronales en la informática\" seguida de la primera Conferencia internacional sobre redes neuronales del Instituto de Ingenieros Eléctricos y Electrónicos (IEEE) en 1987."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V63JQ2hNduEV"
      },
      "source": [
        "Sin embargo, fue un importante redescubrimiento de un concepto que ya existía desde los tiempos del cálculo en varias variables y que ayudó a las redes neuronales a salir de su tumba prematura. El **Backpropagation**, era un método basado en la intuición que atribuía una importancia reducida a cada evento a medida que uno retrocedía en la cadena de eventos. La primera persona que vio su potencial para las redes neuronales y resolvió la pregunta de cómo se traduciría eso para NLP fue __Paul Werbos__, quien, inspirado en parte por su aplicación a la mente humana y el trabajo de Freud sobre el flujo hacia atrás de la asignación de créditos, escribió una tesis doctoral exponiendo su importancia. Sin embargo, nadie en la comunidad notó este trabajo hasta que Parker publicó un informe sobre su trabajo en M.I.T. en 1985. Fue sólo después de ser redescubierto por __Rumelhart, Hinton y Williams__, y republicado en un marco claro y detallado, que la técnica se apoderó de la comunidad.\n",
        "\n",
        "El **Backpropagation** junto con **Gradient Descent** forman la columna vertebral y la potencia de las redes neuronales. Mientras que Gradient Descent actualiza y mueve constantemente los pesos y el sesgo hacia el mínimo de la función de costo, la retropropagación evalúa el gradiente del costo de pesos y sesgos, cuya magnitud y dirección se utiliza mediante el descenso del gradiente para evaluar el tamaño y la dirección de las correcciones a los parámetros de pesos y sesgos.\n",
        "\n",
        "En 1997, __Schmidhuber & Hochreiter__ propuso un marco de red neuronal recurrente, __Long Short-Term Memory (LSTM)__ y en 1998, __Yann LeCun__ publicó Aprendizaje basado en gradientes aplicado al reconocimiento de documentos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdPXqo1fhYY"
      },
      "source": [
        "El impacto del aprendizaje profundo en la industria comenzó a principios de la década de 2000, cuando las CNN ya procesaban entre el 10% y el 20% de todos los cheques emitidos en los Estados Unidos, sin embargo las aplicaciones industriales del aprendizaje profundo para el reconocimiento de voz a gran escala comenzaron alrededor de 2012.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1MlCHqCSfvwbdEdv58Ee3v79Dt0TX1mdI\" width='700'>\n",
        "<br>\n",
        "\n",
        "\n",
        "Entre 2011 y 2012 se realizaron impactos  significativos en el reconocimiento de imágenes u objetos. Aunque las CNN entrenadas por retropropagación habían existido durante décadas y las implementaciones de GPU de NN durante años, incluidas las CNN, se necesitaban implementaciones rápidas de CNN en GPU para avanzar en la visión por computadora.\n",
        "\n",
        "En 2011, este enfoque logró por primera vez un rendimiento sobrehumano en un concurso de reconocimiento de patrones visuales. También en 2011, ganó el concurso de escritura a mano china ICDAR, y en mayo de 2012, ganó el concurso de segmentación de imágenes ISBI. Hasta 2011, las CNN no desempeñaron un papel importante en las conferencias de visión por computadora, pero en junio de 2012, un artículo de Ciresan et al. En la conferencia líder, CVPR [1] mostró cómo la combinación máxima de CNN en GPU puede mejorar drásticamente muchos registros de referencia de visión. En octubre de 2012, un sistema similar de Krizhevsky et al (Alex Net). [2] ganó la competencia ImageNet a gran escala por un margen significativo sobre los métodos de aprendizaje automático poco profundos. En noviembre de 2012, el sistema de Ciresan et al. También ganó el concurso ICPR sobre análisis de imágenes médicas grandes para la detección del cáncer, y al año siguiente también el Gran Desafío MICCAI sobre el mismo tema. En 2013 y 2014, la tasa de error en la tarea de ImageNet que utiliza el aprendizaje profundo se redujo aún más, siguiendo una tendencia similar en el reconocimiento de voz a gran escala. El proyecto Wolfram Image Identification dio a conocer estas mejoras. [100]\n",
        "\n",
        " 1. Ciresan, D.; Meier, U.; Schmidhuber, J. (2012). \"Multi-column deep neural networks for image classification\". 2012 IEEE Conference on Computer Vision and Pattern Recognition. pp. 3642–3649. arXiv:1202.2745. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8. S2CID 2161592.\n",
        "\n",
        "2. Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffry (2012). \"ImageNet Classification with Deep Convolutional Neural Networks\" (PDF). NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='purple' style='bold' size=5>**EXPERIMENTO** </font>"
      ],
      "metadata": {
        "id": "eRCd5oXvlYaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Red neuronal densa en MNIST**\n",
        "\n",
        "Este script demuestra un modelo de red neuronal densa muy básico\n",
        "usando el dataset MNIST (imágenes 28×28) y solo dos capas.\n"
      ],
      "metadata": {
        "id": "4sLR2bn8l-Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# 1. Cargar datos\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 2. Normalizar píxeles a [0,1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test  = x_test.astype('float32')  / 255.0\n",
        "\n",
        "# 3. Definir modelo simple\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),            # Aplanar imagen\n",
        "    Dense(64, activation='relu'),             # Capa oculta\n",
        "    Dense(10, activation='softmax')           # Capa de salida para 10 clases\n",
        "])\n",
        "\n",
        "# 4. Compilar modelo\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',  # etiquetas enteras\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "# 5. Entrenamiento rápido (1 época)\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=1,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# 6. Evaluar en test\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Precisión en test: {accuracy:.4f}\")\n",
        "\n",
        "# Este ejemplo consume poca RAM y se ejecuta en segundos.\n",
        "# Para extenderlo, aumenta épocas o las unidades de la capa oculta.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "JRNZKVrol9kz",
        "outputId": "4cb870c4-0007-45af-b877-5e5c22d275c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "422/422 - 3s - 7ms/step - accuracy: 0.8796 - loss: 0.4466 - val_accuracy: 0.9437 - val_loss: 0.2058\n",
            "Precisión en test: 0.9332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='purple' style='bold' size=5>**FIN EXPERIMENTO** </font>"
      ],
      "metadata": {
        "id": "I0RVs-kClVvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='purple' style='bold' size=5>**MATERIAL ADICIONAL** </font>"
      ],
      "metadata": {
        "id": "5kcbXiislTj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Redes Neuronales Simples y el Dataset MNIST**\n",
        "\n",
        "**¿Qué es el dataset MNIST?**\n",
        "\n",
        "MNIST (Modified National Institute of Standards and Technology database) es una base de datos clásica en machine learning, que contiene 70,000 imágenes en escala de grises de dígitos escritos a mano (del 0 al 9).\n",
        "Fue creado por Yann LeCun, Corinna Cortes y Christopher J.C. Burges en 1998, y ha sido uno de los conjuntos de datos más populares para probar algoritmos de clasificación de imágenes (LeCun et al., 1998).\n",
        "\n",
        "**Referencia:**\n",
        "\n",
        "LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11), 2278–2324. https://doi.org/10.1109/5.726791"
      ],
      "metadata": {
        "id": "AyL3cwj_oGnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Cargar datos\n",
        "(x_train, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "# Mostrar primeros 5 dígitos\n",
        "fig, axes = plt.subplots(1, 5, figsize=(10, 3))\n",
        "for i in range(5):\n",
        "    axes[i].imshow(x_train[i], cmap='gray')\n",
        "    axes[i].set_title(f\"Etiqueta: {y_train[i]}\")\n",
        "    axes[i].axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "kiJl0GPMoRVn",
        "outputId": "bd44de19-211e-4e4c-d73e-e514b5029be2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código carga MNIST, selecciona los primeros 5 dígitos y los muestra con su etiqueta."
      ],
      "metadata": {
        "id": "GDNjNk_YoYot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='purple' style='bold' size=5>**FIN MATERIAL ADICIONAL** </font>"
      ],
      "metadata": {
        "id": "o_C_xr9ElRQK"
      }
    }
  ]
}